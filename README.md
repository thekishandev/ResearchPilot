# ResearchPilot ðŸš€

> Transform 2-8 hour manual research tasks into <10 second AI-synthesized intelligence reports

**FutureStack GenAI Hackathon Submission** (Sept 29 - Oct 5, 2024)

## Overview

ResearchPilot is an AI research copilot that orchestrates 10+ data sources using ultra-fast AI inference to deliver comprehensive research reports in seconds instead of hours.

### Target Users
- Analysts
- Researchers
- Journalists
- Consultants

## ðŸ† Sponsor Technology Integration

### 1. Cerebras API (Primary)# PHASE 0 PROMPT â€” Project Initialization & Repository Setup

## Context
Refer to MASTER PROJECT CONTEXT. We're starting from scratch.

## Your Task
Set up the complete project structure with initial configurations, Docker setup, and development environment.

## Specific Requirements

### 1. Create Project Structure

Generate complete folder structure:
researchpilot/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                    # GitHub Actions
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ init.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”‚   â”œâ”€â”€ init.py
â”‚   â”‚   â”‚   â””â”€â”€ research.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ init.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cerebras_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ mcp_orchestrator.py
â”‚   â”‚   â”‚   â””â”€â”€ llama_local.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ init.py
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ db/
â”‚   â”‚       â”œâ”€â”€ init.py
â”‚   â”‚       â””â”€â”€ database.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ mcp-servers/
â”‚   â”œâ”€â”€ web-search/
â”‚   â”‚   â”œâ”€â”€ server.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ arxiv/
â”‚   â”‚   â”œâ”€â”€ server.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ server.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ filesystem/
â”‚   â”‚   â”œâ”€â”€ server.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ github/
â”‚   â”‚   â”œâ”€â”€ server.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ news/
â”‚       â”œâ”€â”€ server.py
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ gateway/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ interceptors/
â”‚       â”œâ”€â”€ audit.py
â”‚       â”œâ”€â”€ rate_limit.py
â”‚       â””â”€â”€ injection_guard.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.prod.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


### 2. Generate Configuration Files

#### docker-compose.yml
```yaml
version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: researchpilot
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Backend API
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/researchpilot
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY}
      - OLLAMA_HOST=http://ollama:11434
      - MCP_GATEWAY_URL=http://mcp-gateway:3000
    depends_on:
      postgres:
        condition: service_healthy
      mcp-gateway:
        condition: service_started
    volumes:
      - ./backend:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # Frontend
  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    volumes:
      - ./frontend:/app
      - /app/node_modules
    command: npm run dev -- --host

  # Docker MCP Gateway
  mcp-gateway:
    image: docker/mcp-gateway:latest
    ports:
      - "3000:3000"
    environment:
      - MCP_LOG_LEVEL=info
    volumes:
      - ./gateway/config.json:/config/gateway-config.json
      - ./gateway/interceptors:/interceptors
    depends_on:
      - mcp-web-search
      - mcp-arxiv
      - mcp-database
      - mcp-filesystem
      - mcp-github
      - mcp-news

  # MCP Servers
  mcp-web-search:
    build: ./mcp-servers/web-search
    environment:
      - SEARCH_PROVIDER=duckduckgo

  mcp-arxiv:
    build: ./mcp-servers/arxiv

  mcp-database:
    build: ./mcp-servers/database
    environment:
      - DB_URL=postgresql://postgres:postgres@postgres:5432/researchpilot
    depends_on:
      postgres:
        condition: service_healthy

  mcp-filesystem:
    build: ./mcp-servers/filesystem
    volumes:
      - ./research-docs:/docs

  mcp-github:
    build: ./mcp-servers/github
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}

  mcp-news:
    build: ./mcp-servers/news
    environment:
      - NEWS_API_KEY=${NEWS_API_KEY}

  # Ollama for local Llama inference
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
  postgres_data:
  ollama_data:

- **Ultra-fast inference** using Llama 3.3 70B
- **Streaming responses** for real-time UX
- **Sub-2 second synthesis** of multi-source results
- Model: `llama-3.3-70b`

### 2. Meta Llama (Secondary)
- **Cloud:** Llama 3.3 70B via Cerebras for main synthesis
- **Edge:** Llama 3.1 8B via Ollama for credibility scoring
- Demonstrates both cloud and edge deployment

### 3. Docker MCP Gateway (Primary)
- Orchestrates **6+ MCP servers**
- Custom security interceptors (audit, rate-limit, injection prevention)
- Unified endpoint for all data sources

## ðŸ—ï¸ Architecture

```
Frontend (React/TS)
    â†“ HTTP/SSE
Backend (FastAPI)
    â†“ Orchestrates
    â”œâ†’ Cerebras API (synthesis)
    â””â†’ Docker MCP Gateway
        â†“ Routes to
        â”œâ†’ Web Search MCP Server
        â”œâ†’ ArXiv MCP Server
        â”œâ†’ Database MCP Server
        â”œâ†’ Filesystem MCP Server
        â”œâ†’ GitHub MCP Server
        â””â†’ News MCP Server
```

## ðŸ› ï¸ Technology Stack

### Frontend
- React 18 + TypeScript 5
- Vite build tool
- Tailwind CSS + shadcn/ui
- Server-Sent Events (SSE)
- React Query

### Backend
- Python 3.11+ with FastAPI
- Asyncio for parallel operations
- SQLAlchemy 2.0
- Pydantic v2
- SSE streaming

### Infrastructure
- PostgreSQL 15
- Docker Compose
- Ollama (local Llama)
- Docker MCP Gateway

## ðŸ“Š MCP Data Sources (6 Total)

1. **Web Search** - DuckDuckGo integration
2. **ArXiv Papers** - Academic research
3. **PostgreSQL Database** - Cached research + analytics
4. **Filesystem/Documents** - Local document search
5. **GitHub Code Search** - Code repository analysis
6. **News API** - Current news aggregation

## ðŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Node.js 18+
- Python 3.11+
- Ollama (for local Llama inference)

### 1. Clone Repository
```bash
git clone https://github.com/yourusername/ResearchPilot.git
cd ResearchPilot
```

### 2. Environment Setup
```bash
# Copy environment template
cp .env.example .env

# Add your API keys
# - CEREBRAS_API_KEY
# - NEWS_API_KEY
# - GITHUB_TOKEN (optional)
```

### 3. Start Infrastructure
```bash
# Start all services
docker-compose up -d

# Verify services
docker-compose ps
```

### 4. Install Dependencies

**Backend:**
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

**Frontend:**
```bash
cd frontend
npm install
```

### 5. Run Application

**Backend (Terminal 1):**
```bash
cd backend
uvicorn app.main:app --reload --port 8000
```

**Frontend (Terminal 2):**
```bash
cd frontend
npm run dev
```

**Access:**
- Frontend: http://localhost:5173
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

## ðŸ“ Project Structure

```
researchpilot/
â”œâ”€â”€ frontend/                 # React + TypeScript
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/      # UI components
â”‚   â”‚   â”œâ”€â”€ hooks/           # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ lib/             # Utilities
â”‚   â”‚   â”œâ”€â”€ types/           # TypeScript types
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ backend/                 # FastAPI
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ core/           # Core logic
â”‚   â”‚   â”œâ”€â”€ models/         # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ mcp-servers/            # MCP server implementations
â”‚   â”œâ”€â”€ web-search/
â”‚   â”œâ”€â”€ arxiv/
â”‚   â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ filesystem/
â”‚   â”œâ”€â”€ github/
â”‚   â””â”€â”€ news/
â”œâ”€â”€ gateway/                # MCP Gateway config
â”‚   â””â”€â”€ config.json
â”œâ”€â”€ docker-compose.yml      # Local orchestration
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ðŸŽ¯ Key Features

### 1. Ultra-Fast Research Synthesis
- Parallel data source querying
- Sub-2 second response time
- Real-time streaming results

### 2. Multi-Source Intelligence
- 6+ simultaneous data sources
- Automatic source credibility scoring
- Graceful degradation on source failures

### 3. Security & Reliability
- Custom MCP Gateway interceptors
- Rate limiting & audit logging
- SQL injection prevention
- No secrets in code (environment variables)

### 4. Rich User Experience
- Real-time streaming updates
- Source attribution
- Credibility scores
- Export to multiple formats

## ðŸ“ˆ Performance Metrics

- **Response Time:** <2 seconds for 5-source synthesis
- **Source Coverage:** 6+ simultaneous MCP servers
- **Uptime:** Graceful degradation with fallbacks
- **Code Quality:** TypeScript strict mode, Python type hints

## ðŸ§ª Testing

```bash
# Backend tests
cd backend
pytest tests/ -v --cov=app

# Frontend tests
cd frontend
npm test
```

## ðŸ“¦ Deployment

### Backend (Render/Railway)
```bash
# Deploy to Render
render deploy

# Or Railway
railway up
```

### Frontend (Vercel/Netlify)
```bash
# Deploy to Vercel
vercel deploy --prod

# Or Netlify
netlify deploy --prod
```

## ðŸ”§ Configuration

### Environment Variables

**Backend (.env):**
```env
CEREBRAS_API_KEY=your_cerebras_key
DATABASE_URL=postgresql://user:pass@localhost:5432/researchpilot
OLLAMA_HOST=http://localhost:11434
MCP_GATEWAY_URL=http://localhost:8080
NEWS_API_KEY=your_news_api_key
GITHUB_TOKEN=your_github_token
```

**Frontend (.env):**
```env
VITE_API_URL=http://localhost:8000
```

## ðŸ“– API Documentation

Once running, visit:
- **Swagger UI:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc

### Key Endpoints

```
POST /api/v1/research/query
- Submit research query
- Returns: Streaming SSE response

GET /api/v1/research/{id}
- Retrieve completed research
- Returns: JSON report

GET /api/v1/sources/status
- Check MCP server health
- Returns: Source availability status
```

## ðŸ¤ Contributing

This is a hackathon submission. For collaboration inquiries, please open an issue.

## ðŸ“„ License

MIT License - See LICENSE file for details

## ðŸ™ Acknowledgments

- **Cerebras** for ultra-fast inference
- **Meta** for Llama models
- **Docker** for MCP Gateway
- **FutureStack** for hosting the hackathon

## ðŸ“§ Contact

For questions or demo requests, reach out via GitHub issues.

---

**Built for FutureStack GenAI Hackathon 2024** ðŸš€
